# Motivation and Goal
## Video-to-sound generation.


# Development Route
# Speech
## Early studies of the speech reconstruction task proposed to estimate the spectral envelope from the hand-crafted visual feature. Then the spectral envelope is then used in the audio generation model to synthetic corresponding audio signal [1,2]. 
## The subsequent works designed the end-to-end models, extracting visual features from raw data [3,4]. Akbari et al. [5] tried to reconstruct more natural sounding speech by learning a mapping between the face and auditory spectrogram instead of the spectral envelope like Linear Predictive Coding, improving speech intelligibility and quality. 
## To avoid the burden of learning audio-related intermediate representation, the Generative Adversarial Networks (GAN) based model is proposed to directly synthesize audio waveform from silent videos [6,7]. 
## Considering video of one single view is hard to capture complete information, multi-view videos are further adapted to improve generation quality [8,9]. 

# Music 
## The early studies of music generation used the traditional computer vision techniques to extract the visual information [10] and have been outperformed by deep learning based methods [11,12,13,14], since the neural network is more robust to the variant of environment. 
## Recently, some deep generative networks, including Variational Autoencoder and GANs, are also introduced in the generation of more spread music scenes [15,16]. 
## The above methods aim to produce determined music based on strict audio-visual correspondence. While recently, Di et al. [17] leveraged the rhythmic relations between video and music at the semantic level to generate non-determined background music, matching the given video.

# Natural Sound
## Owens et al. [18] pioneered the natural sound generation task via collecting the Greatest Hits dataset and presented a model that generates sound based on silent videos of hitting or scratching objects. 
## Subsequently, Zhou et al. [19] employed the SampleRNN model to generate raw waveform samples given input video frames, covering ambient sounds and sounds of animals or peoples. 
## Chen et al. [20] took the object-irrelevant background sound into consideration, achieving high-quality results. 
## Recently, Iashin et al. [21] presented an efficient visual-driven generation method via a codebook representation trained by a variant of GANs.


[1] [Reconstructing intelligible audio speech from visual speech features](https://www.isca-speech.org/archive_v0/interspeech_2015/papers/i15_3355.pdf)
[2] [Generating Intelligible Audio Speech From Visual Speech](https://ieeexplore.ieee.org/document/7949073/)
[3] [Vid2speech: Speech Reconstruction From Silent Video](https://ieeexplore.ieee.org/abstract/document/7953127)
[4] [Improved Speech Reconstruction From Silent Video](https://openaccess.thecvf.com/content_ICCV_2017_workshops/w8/html/Ephrat_Improved_Speech_Reconstruction_ICCV_2017_paper.html)
[5] Lip2Audspec: Speech Reconstruction from Silent Lip Movements Video](https://ieeexplore.ieee.org/abstract/document/8461856)
[6] [Video-Driven Speech Reconstruction using Generative Adversarial Networks](https://arxiv.org/abs/1906.06301)
[7] [End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks](https://ieeexplore.ieee.org/abstract/document/9760273)
[8] [Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed](https://arxiv.org/abs/1807.00619)
[9] [Hush-Hush Speak: Speech Reconstruction Using Silent Videos](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/3269.pdf)
[10] [Real-Time Piano Music Transcription Based on Computer Vision](https://ieeexplore.ieee.org/abstract/document/7225173)
[11] [Deep Cross-Modal Audio-Visual Generation](https://dl.acm.org/doi/abs/10.1145/3126686.3126723)
[12] [Audeo: Audio Generation for a Silent Performance Video](https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html)
[13] [Foley Music: Learning to Generate Music from Videos](https://link.springer.com/chapter/10.1007/978-3-030-58621-8_44)
[14] [Sight to Sound: An End-to-End Approach for Visual Piano Transcription](https://ieeexplore.ieee.org/abstract/document/9053115)
[15] [Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements](https://arxiv.org/abs/2012.03478)
[16] [Collaborative Learning to Generate Audio-Video Jointly](https://ieeexplore.ieee.org/abstract/document/9413802/)
[17] [Video Background Music Generation with Controllable Music Transformer](https://dl.acm.org/doi/abs/10.1145/3474085.3475195)
[18] [Visually Indicated Sounds](https://openaccess.thecvf.com/content_cvpr_2016/html/Owens_Visually_Indicated_Sounds_CVPR_2016_paper.html)
[19] [Visual to Sound: Generating Natural Sound for Videos in the Wild](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html)
[20] [Generating Visually Aligned Sound From Videos](https://ieeexplore.ieee.org/abstract/document/9151258)
[21] [Taming Visually Guided Sound Generation](https://arxiv.org/abs/2110.08791)