# Motivation and Goal
## Sound-to-video generation.


# Development Route
# talking face
## The early talking face generation methods are speaker-dependent and rely on a large video corpus of the target person [1,2]. 
## Audio-driven face generation methods are presented to synthesize speaker-independent talking face [3,4,5,6].
## When a person is speaking, the movement of lips is the most visible, but other parts, especially facial expression and head pose, are also variable. Hence, researchers introduce structural information such as landmark [7,8,9] 3D model [10,11,12] to model the head pose as well as facial expression. 
##  Zhou et al. [13] further considered an implicit low-dimension pose code to generate the pose-controllable talking face without the assistance of structural information, avoiding the performance degradation resulting in inaccuracy estimated structural representation.
## Recently, since farcical emotion is also an important feature of humans, the emotion factor is considered in the talking generation task to get more fine-grained results [14,15]. 

# Gesture
## In the early sage, the gesture-speech alignment and generation methods are mostly rule-based, resulting in the generated gesture being limited to a selected discrete set [16,17]. 
## Alongside the advance of deep learning, more methods begin to use the data-driven scheme to fully model the gesture pattern of different speakers, utilizing 2D [18,19] or 3D pose model [20,21]. 
## The early methods formulated this task as a classification task [22], while more recent works consider it as the regression task to generate continuous motion [23,24]. 
## Since each speaker can own a specific gesture style, Ginosar et al. [25] proposed to model the speaker-specific style during generation. Ahuja et al. [26] further transferred the learnt gesture style of one speaker to another one via disentangling the style and content of gestures. 
## Alexanderson et al.[23] presented a generative model to produce different but plausible gestures given the same input speech just like the actual human. 
## Besides gesture style, the content of speech is also focused on by many researchers [24,27]. 
## Recently, Liang et al. [19] decoupled speech information into semantic-relevant cues and semantic-irrelevant cues to explicitly learn and produce semantic-aware gestures. 

# Dance
## The early works of dance generation conditioned music tackle this task as the retrieval problem that mainly considered the motion-music similarity, blocking their creativity [28,29,30]. 
## Later, machine learning models, like Long Short-Term Memory (LSTM), was widely used to predict motion and pose given a music [31,32].
## Recently, the dance generation task has been formulated as a generative perspective and achieved promising performance [33,34,35]. 
## Further, Huang et al.[36] took the music genre into consideration beyond the correlation between motion and music beats as well as rhythm. They constructed a transformer-based architecture to generate dance based on music with a specific music genre. In a nutshell, the sound-to-video generation has produced many extraordinary works, which have the potential to facilitate a wide range of applications in practice.


[1] [Realistic Facial Expression Synthesis for an Image-based Talking Head](https://ieeexplore.ieee.org/document/6011835)
[2] [Synthesizing Obama: learning lip sync from audio](https://dl.acm.org/doi/abs/10.1145/3072959.3073640)
[3] [Lip Movements Generation at a Glance](https://openaccess.thecvf.com/content_ECCV_2018/html/Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper.html)
[4] [You Said That?: Synthesising Talking Faces from Audio](https://link.springer.com/article/10.1007/s11263-019-01150-y)
[5] [Realistic Speech-Driven Facial Animation with GANs](https://link.springer.com/article/10.1007/s11263-019-01251-8)
[6] [GANimation: One-Shot Anatomically Consistent Facial Animation](https://link.springer.com/article/10.1007/s11263-019-01210-3)
[7] [Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html)
[8] [Makelttalk: Speaker-Aware Talking-Head Animation](https://dl.acm.org/doi/abs/10.1145/3414685.3417774)
[9] [FReeNet: Multi-Identity Face Reenactment](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_FReeNet_Multi-Identity_Face_Reenactment_CVPR_2020_paper.html)
[10] [Neural Voice Puppetry: Audio-driven Facial Reenactment](https://justusthies.github.io/posts/neural-voice-puppetry/)
[11] [Rotate-and-Render: Unsupervised Photorealistic Face Rotation from Single-View Images](https://arxiv.org/abs/2003.08124)
[12] [Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation](https://ojs.aaai.org/index.php/AAAI/article/view/16286)
[13] [Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation](https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.html)
[14] [MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660698.pdf)
[15] [Audio-Driven Emotional Video Portraits](https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.html)
[16] [BEAT: the Behavior Expression Animation Toolkit](https://link.springer.com/chapter/10.1007/978-3-662-08373-4_8)
[17] [Robot behavior toolkit: Generating effective social behaviors for robots](https://ieeexplore.ieee.org/document/6249556/)
[18] [Learning Individual Styles of Conversational Gesture](https://openaccess.thecvf.com/content_CVPR_2019/html/Ginosar_Learning_Individual_Styles_of_Conversational_Gesture_CVPR_2019_paper.html)
[19] [SEEG: Semantic Energized Co-Speech Gesture Generation](https://openaccess.thecvf.com/content/CVPR2022/html/Liang_SEEG_Semantic_Energized_Co-Speech_Gesture_Generation_CVPR_2022_paper.html)
[20] [Analyzing Input and Output Representations for Speech-Driven Gesture Generation](https://dl.acm.org/doi/abs/10.1145/3308532.3329472)
[21] [Evaluation of Speech-to-Gesture Generation Using Bi-Directional LSTM Network](https://dl.acm.org/doi/abs/10.1145/3267851.3267878)
[22] [To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations](https://dl.acm.org/doi/abs/10.1145/3340555.3353725)
[23] [Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows](https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13946)
[24] [Gesticulator: A Framework For Semantically-Aware Speech-Driven Gesture Generation](https://dl.acm.org/doi/abs/10.1145/3382507.3418815)
[25] [Learning Individual Styles of Conversational Gesture](https://openaccess.thecvf.com/content_CVPR_2019/html/Ginosar_Learning_Individual_Styles_of_Conversational_Gesture_CVPR_2019_paper.html)
[26] [Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach](https://arxiv.org/abs/2007.12553)
[27] [Speech Gesture Generation From The Trimodal Context Of Text, Audio, And Speaker Identity](https://dl.acm.org/doi/abs/10.1145/3414685.3417838)
[28] [Example-Based Automatic Music-Driven Conventional Dance Motion Synthesis](https://ieeexplore.ieee.org/document/5753889)
[29] [Learn2dance: Learning Statistical Music-to-dance Mappings for Choreography Synthesis](http://home.ku.edu.tr/~yyemez/IEEETrMultimedia12.pdf)
[30] [Music Similarity-based Approach to Generating Dance Motion Sequence](https://link.springer.com/article/10.1007/s11042-012-1288-5)
[31] [Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis](https://dl.acm.org/doi/abs/10.1145/3240508.3240526)
[32] [Audio to Body Dynamics](https://openaccess.thecvf.com/content_cvpr_2018/html/Shlizerman_Audio_to_Body_CVPR_2018_paper.html)
[33] [Dancing to Music](https://proceedings.neurips.cc/paper/2019/hash/7ca57a9f85a19a6e4b9a248c1daca185-Abstract.html)
[34] [Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning](https://arxiv.org/abs/2006.06119)
[35] [AI Choreographer: Music Conditioned 3D Dance Generation With AIST++](https://openaccess.thecvf.com/content/ICCV2021/html/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.html)
[36] [Genre-Conditioned Long-Term 3D Dance Generation Driven by Music](https://ieeexplore.ieee.org/abstract/document/9747838/)
