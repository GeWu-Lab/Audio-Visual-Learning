# Motivation and Goal
## Audio-visual modalities share related knowledge. 
## Distilling knowledge from ones to others.


# Development Route
## To avoid the expensive and time-consuming labelling process, Aytar et al. [1] designed a teacher-student network to train the student audio model using the large-scale and economically acquired unlabelled videos under the supervision of a pre-trained vision teacher. 
## Gan et al. [2] also employed the audio-visual correlation to training the stereo sound student model by transferring the knowledge of the visual teacher for the vehicle tracking task. Their model can independently accomplish the object localization purely based on stereo sound during the test. 
## Considering these methods only use the RGB visual teacher during training, which limits their performance since RGB modality is vulnerable to the variants of factors like weather and illumination, Valverde et al. [3] integrated multiple visual teacher networks, including RGB, depth, and thermal, to fully utilize the complementary visual cues for improving the robustness of audio student network. 
## Similarly, Yin et al. [4] designed a robust audio-visual teacher network to fuse complementary information of multiple modalities, facilitating the learning of the visual student network. 
## Besides increasing the number of teachers, Zhang et al. [5] enhanced the distillation performance by distilling the knowledge of audio-visual teachers at three different levels: label-level, embedding-level and distribution level. 
## It is known that the performance of the student is hard to outperform its teacher, for which the above methods combine multiple teachers for better generalization. In contrast, Xue et al. [6] used a uni-modal teacher to train a multi-modal student, observing an interesting phenomenon: the multi-modal student model generalizes better than its uni-modal teacher. They argued that the multi-modal student is capable of enhancing pseudo labels given by the teacher empirically and theoretically.


[1] [SoundNet: Learning Sound Representations from Unlabeled Video](https://proceedings.neurips.cc/paper/2016/hash/7dcd340d84f762eba80aa538b0c527f7-Abstract.html)
[2] [Self-Supervised Moving Vehicle Tracking With Stereo Sound](https://openaccess.thecvf.com/content_ICCV_2019/html/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.html)
[3] [There Is More Than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking With Sound by Distilling Multimodal Knowledge](https://openaccess.thecvf.com/content/CVPR2021/html/Valverde_There_Is_More_Than_Meets_the_Eye_Self-Supervised_Multi-Object_Detection_CVPR_2021_paper.html?ref=https://githubhelp.com)
[4] [Enhanced Audio Tagging via Multi- to Single-Modal Teacher-Student Mutual Learning](https://ojs.aaai.org/index.php/AAAI/article/view/17280)
[5] [Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification](https://drive.google.com/file/d/1ISP0ot1WFxZz_IhTAOiWu4nT-Q1bXWtH/view)
[6] [Multimodal Knowledge Expansion](https://openaccess.thecvf.com/content/ICCV2021/html/Xue_Multimodal_Knowledge_Expansion_ICCV_2021_paper.html)