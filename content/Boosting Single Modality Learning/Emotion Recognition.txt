# Motivation of audio-visual input
## Uni-modal information is not comprehensive to represent emotion.
## Facial expression as well as vocal mainly contribute to human sentiment;

# Development Route
## At the early stage of the emotion recognition study, features of multiple modalities are generated by hand-crafted technique. The audio features are often extracted based on the acoustic characteristic, like pitch and energy [1,2,3], while visual features are often based on facial texture (e.g., wrinkles, furrows) or components (e.g., eyebrows, mouth, eyes) [4]. The fusion operations of these studies can be categorized into feature-level fusion [5,6], decision-level fusion [7,8],and model-level fusion [9,10]. 
## More recently, the more powerful deep neural network is widely employed and more dense inter-, as well as intra-modal fusion, is performed in the emotion recognition task [11,12,13,14,15]. 
## Considering the display of emotions is continuous in temporal, context information is also taken into consideration [16,17]. 
## Also, the transformer-based framework is also introduced since its advantage in capturing global attention [18]. 
## Some work has been done on more complex sentiments such as sarcasm and humor [19,20,21].


[1] [Emotion Recognition Based on Joint Visual and Audio Cues](https://ieeexplore.ieee.org/document/1699090)
[2] [Error Weighted Semi-Coupled Hidden Markov Model for Audio-Visual Emotion Recognition](https://ieeexplore.ieee.org/document/6042338)
[3] [Two-Level Hierarchical Alignment for Semi-Coupled HMM-Based Audiovisual Emotion Recognition With Temporal Course](https://ieeexplore.ieee.org/abstract/document/6542683)
[4] [Automatic Facial Expression Analysis: a Survey](https://www.sciencedirect.com/science/article/pii/S0031320302000523)
[5] [Context-sensitive Learning for Enhanced Audiovisual Emotion Classification](https://ieeexplore.ieee.org/document/7344611)
[6] [Audiovisual Vocal Outburst Classification in Noisy Acoustic Conditions](https://ieeexplore.ieee.org/document/6289067)
[7] [Decision Level Combination of Multiple Modalities for Recognition and Analysis of Emotional Expression](https://ieeexplore.ieee.org/abstract/document/549489)
[8] [Modeling Latent Discriminative Dynamic of Multi-dimensional Affective Signals](https://link.springer.com/chapter/10.1007/978-3-642-24571-8_51)
[9] [Audio-visual Affective Expression Recognition through Multistream Fused HMM][https://ieeexplore.ieee.org/document/4523967]
[10] [Audio Visual Emotion Recognition Based on Triple-Stream Dynamic Bayesian Network Models](https://link.springer.com/chapter/10.1007/978-3-642-24600-5_64)
[11] [Tensor Fusion Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/1707.07250)
[12] [Multi-attention Recurrent Network for Human Communication Comprehension](https://ojs.aaai.org/index.php/AAAI/article/view/12024)
[13] [Memory Fusion Network for Multi-view Sequential Learning](https://ojs.aaai.org/index.php/AAAI/article/view/12021)
[14] [Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7098709/)
[15] [Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences](https://openaccess.thecvf.com/content/CVPR2021/html/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.html)
[16] [Contextual Inter-modal Attention for Multi-modal Sentiment Analysis](https://aclanthology.org/D18-1382/)
[17] [Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation](https://arxiv.org/abs/2002.08267)
[18] [A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis](https://aclanthology.org/2020.challengehml-1.1/)
[19] [Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model](https://aclanthology.org/P19-1239/?ref=https://githubhelp.com)
[20] [Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis](https://aclanthology.org/2020.acl-main.401/)
[21] [Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations](https://ieeexplore.ieee.org/abstract/document/9442359)
