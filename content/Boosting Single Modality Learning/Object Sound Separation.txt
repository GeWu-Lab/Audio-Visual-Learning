# Motivation of audio-visual input
## Signal of multiple objects in aduio is overlapped. Mixture of objects with the same class is hard to be split based on audio input.
## Signal of multiple objects in visual is isolated.
## Motion and sound of the same object is consistent.


# Development Route
## The classical methods of object sound separation [1,2,3,4] have been outperformed by deep learning based frameworks in recent years [5,6,7]. 
## Zhao et al.[8] further took the trajectory motion cues into consideration and improved separation quality apparently and maintained its effectiveness for separating sounds of the same kind of objects. 
## What's more, to model the fine-grained interaction between human and object, especially in the musical scene, Gao et al.[9] introduced the keypoint-based structured representation and achieved impressive performance. 
## One challenge in sound separation is that the number of sounding objects is fixed as a prior. To solve it, the MinusPlus Network [10] that recursively separates sound components is proposed to deal with the sound mixtures with arbitrary numbers and types of sounding objects. 
## Another challenge is that most mentioned methods utilize the mix-and-separate framework and train the model with synthetic multi-source videos, bringing the additional training burden. Hence, the co-separation training paradigm that learns the correlation between similar-looking objects and sounds across videos is presented [11]. 
## Recently, considering the fact that the same object may produce different sounds of distinct interactions, Chatterjee et al. [12] formulated the visual scene as a graph for better separation, allowing to model the characteristics of objects and their interactions. Tian et al. [13] further extended the separation from audio to visual, grounding objects both in audio and visual with a unified framework.


[1] [Non-negative Matrix Factor Deconvolution; Extraction of Multiple Sound Sources from Monophonic Inputs](https://link.springer.com/chapter/10.1007/978-3-540-30110-3_63)
[2] [Harmony in Motion](https://ieeexplore.ieee.org/abstract/document/4270342/)
[3] [Blind Audiovisual Source Separation Based on Sparse Redundant Representations](https://hal.inria.fr/inria-00541412/document)
[4] [Motion Informed Audio Source Separation](https://ieeexplore.ieee.org/document/7951787)
[5] [Learning to Separate Object Sounds by Watching Unlabeled Video](https://openaccess.thecvf.com/content_ECCV_2018/html/Ruohan_Gao_Learning_to_Separate_ECCV_2018_paper.html)
[6] [The Sound of Pixels](https://openaccess.thecvf.com/content_ECCV_2018/html/Hang_Zhao_The_Sound_of_ECCV_2018_paper.html)
[7] [Self-supervised Audio-visual Co-segmentation](https://ieeexplore.ieee.org/abstract/document/8682467)
[8] [The Sound of Motions](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_The_Sound_of_Motions_ICCV_2019_paper.html)
[9] [Music Gesture for Visual Sound Separation](https://openaccess.thecvf.com/content_CVPR_2020/html/Gan_Music_Gesture_for_Visual_Sound_Separation_CVPR_2020_paper.html)
[10] [Recursive Visual Sound Separation Using Minus-Plus Net](https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Recursive_Visual_Sound_Separation_Using_Minus-Plus_Net_ICCV_2019_paper.html)
[11] [Co-Separating Sounds of Visual Objects](https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Co-Separating_Sounds_of_Visual_Objects_ICCV_2019_paper.html)
[12] [Visual Scene Graphs for Audio Source Separation](https://openaccess.thecvf.com/content/ICCV2021/html/Chatterjee_Visual_Scene_Graphs_for_Audio_Source_Separation_ICCV_2021_paper.html)
[13] [Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation](https://openaccess.thecvf.com/content/CVPR2021/html/Tian_Cyclic_Co-Learning_of_Sounding_Object_Visual_Grounding_and_Sound_Separation_CVPR_2021_paper.html)

