# Motivation of audio-visual input
## Vision information is limited in the description of actions.
## Many actions company with sound.
## Audio reveals the physical characteristics and reflects the out-of-screen sounding actions.


# Development Route
## Decision fusion and late fusion are commonly used to integrate the audio-visual information to classification [1,2].
## More fine-grained mid-level fusion methods are proposed to fully explore the audio-visual correlation in temporal [3,4].
## In these years, the flourish of transformer has inspired the transformer-based integration mechanism that both considers self-attention and cross-modal attention [5].
## Since videos often contain action-irrelevant segments, making unnecessary computation cost and even possibly interfering with the prediction, audio modality is used to reduce redundancy segments of videos [6,7].
## The modality selection or dropout strategy is adopted for for efficient video recognition [8,9].
## Apart from the above methods that fusing multiple information, some researchers consider the audio modality as the assistance for the domain generalization in visual, alleviating the domain shift problem in action recognition [10,11,12].


[1] [Exploring Multimodal Video Representation For Action Recognition](https://ieeexplore.ieee.org/abstract/document/7727435)
[2] [The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary](https://arxiv.org/abs/1808.03766)
[3] [EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition](https://openaccess.thecvf.com/content_ICCV_2019/html/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.html)
[4] [Audiovisual SlowFast Networks for Video Recognition](https://arxiv.org/abs/2001.08740)
[5] [MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition](https://openaccess.thecvf.com/content/WACV2022/html/Chen_MM-ViT_Multi-Modal_Video_Transformer_for_Compressed_Video_Action_Recognition_WACV_2022_paper.html)
[6] [SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition](https://openaccess.thecvf.com/content_ICCV_2019/html/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.html)
[7] [Listen to Look: Action Recognition by Previewing Audio](https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Listen_to_Look_Action_Recognition_by_Previewing_Audio_CVPR_2020_paper.html)
[8] [AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition](https://openaccess.thecvf.com/content/ICCV2021/html/Panda_AdaMML_Adaptive_Multi-Modal_Learning_for_Efficient_Video_Recognition_ICCV_2021_paper.html)
[9] [Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos](https://openaccess.thecvf.com/content/CVPR2022/html/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.html)
[10] [Cross-Domain First Person Audio-Visual Action Recognition through Relative Norm Alignment](https://arxiv.org/abs/2106.01689)
[11] [Domain Generalization Through Audio-Visual Relative Norm Alignment in First Person Action Recognition](https://openaccess.thecvf.com/content/WACV2022/html/Planamente_Domain_Generalization_Through_Audio-Visual_Relative_Norm_Alignment_in_First_Person_WACV_2022_paper.html)
[12] [Audio-Adaptive Activity Recognition Across Video Domains](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Audio-Adaptive_Activity_Recognition_Across_Video_Domains_CVPR_2022_paper.html)