# Motivation of audio-visual input
## Audio is sensitive to acoustic noise.
## Audio-visual information is complementary.


# Development Route
## In early studies, the visual features are extracted via prior lip-shape representation framework based on lip contours [1,2] or some low-level visual algorithms based on the shape of the mouth [3].
## Hidden Markov model (HMM)-related methods are generally utilized in the modelling of audio-visual synchrony state over time [4,5,6]. 
## Hand-crafted features either require fine labelling or are vulnerable to factors such as illumination, and have been replaced by the later rising neural network methods [7,8,9].
## The transformer-based recognition framework is introduced to build context dependence as well as cross-modal association [10,11,12].
## [13] first attempted to build the audio-visual speech recognition model with unlabeled data via based on the clustering algorithm. 


[1] [Visual Speech Recognition Using Active Shape Models and Hidden Markov Models](https://ieeexplore.ieee.org/document/543246)
[2] [Audiovisual Speech Processing](https://www.cambridge.org/core/books/audiovisual-speech-processing/EFC6432E42C4CD096F899D3F93AEE628)
[3] [A Comparison of Model and Transform-based Visual Features for Audio-visual LVCSR](https://ieeexplore.ieee.org/document/1237849)
[4] [Audio-visual Speech Modeling for Continuous Speech Recognition](https://ieeexplore.ieee.org/document/865479/)
[5] [Dynamic Bayesian Networks for Audio-visual Speech Recognition](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.7441&rep=rep1&type=pdf)
[6] [Dynamic Modality Weighting for Multi-stream hmms in Audio-visual Speech Recognition](https://dl.acm.org/doi/10.1145/1452392.1452442)
[7] [Audio-visual Speech Recognition Using Deep Learning](https://link.springer.com/article/10.1007/s10489-014-0629-7)
[8] [End-To-End Audiovisual Fusion With LSTMs](https://arxiv.org/abs/1709.04343)
[9] [Lip Reading Sentences in the Wild](https://openaccess.thecvf.com/content_cvpr_2017/html/Chung_Lip_Reading_Sentences_CVPR_2017_paper.html)
[10] [Deep Audio-visual Speech Recognition](https://ieeexplore.ieee.org/abstract/document/8585066)
[11] [Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection](https://arxiv.org/abs/1912.11637)
[12] [Multimodal Sparse Transformer Network for Audio-V\visual Speech Recognition](https://ieeexplore.ieee.org/abstract/document/9755926)
[13] [Robust Self-Supervised Audio-V\visual Speech Recognition](https://arxiv.org/abs/2201.01763)

