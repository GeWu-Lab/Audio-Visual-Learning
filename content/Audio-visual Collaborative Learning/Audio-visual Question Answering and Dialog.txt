# Motivation and Goal
## Modelling human scene-aware reasoning.
# Conversing based on audio-visual information.


# Development Route
## Comprehensively taking both audio and visual modality into consideration is promising to advance the modelling of human-like scene perception ability, for which the audio-visual question answering task emerge recently~\cite{yun2021pano,li2022learning}. Yun et al. [1] brought up the Pano-AVQA dataset, containing 360 videos and corresponding question-answer pairs. The Pano-AVQA dataset covers two types of question-answer pairs: spherical spatial relation and audio-visual relation, to better explore the semantic understanding of the panoramic scenes. 
## Li et al. [2] proposed the large-scale MUSIC-AVQA dataset to facilitate spatial-temporal reasoning under dynamic and long-term audio-visual scenes. They designed a transformer-based framework to fully leverage all modalities with inter- and intra-modal attention. Besides, Li et al. [2] released the MUSIC-AVQA dataset that contains question-answer pairs covering different modalities and question types. The audio-visual scenes in MUSIC-AVQA are mainly musical performances which is a typical multi-modal scenario with abundant audio-visual components and their interaction. They proposed a question-answering framework with spatial-temporal grounding to fully exploit and precept the audio-visual scenes. 
## The audio-visual scene-aware dialog task is introduced to train the agent conversing with humans about a temporally varying audio-visual scene using natural, conversational language [3,4,5]. 
## Alamri et al. [3] first collected the Audio Visual Scene-Aware Dialog (AVSD) dataset each sample contains a scene-aware dialog about the video. They provided a simple baseline that leveraged the history dialog and audio-visual input to rank candidate answers. 
## Schwartz et al. [6] further adopted an end-to-end training strategy that first uses a multi-modal attention mechanism to capture cross-modal interactions, and then generates the answer with the Ans-Generation LSTM module. 
## To model fine-grained visual scene information, Geng et al. [7] presented a dynamic scene graph representation learning pipeline and performed inter- and intra-frame reasoning to fully exploit temporal cues. 
## In recent years, the audio-visual scene-aware dialog task has attracted wide attention and been an important track of the Dialog System Technology Challenge (DSTC) [8]. 


[1] [Pano-AVQA: Grounded Audio-Visual Question Answering on 360deg Videos](https://openaccess.thecvf.com/content/ICCV2021/html/Yun_Pano-AVQA_Grounded_Audio-Visual_Question_Answering_on_360deg_Videos_ICCV_2021_paper.html)
[2] [Learning To Answer Questions in Dynamic Audio-Visual Scenarios](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.html)
[3] [Audio Visual Scene-Aware Dialog](https://openaccess.thecvf.com/content_CVPR_2019/html/Alamri_Audio_Visual_Scene-Aware_Dialog_CVPR_2019_paper.html)
[4] [Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog](https://www.merl.com/publications/docs/TR2019-097.pdf)
[5] [End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features](https://ieeexplore.ieee.org/abstract/document/8682583)
[6] [A Simple Baseline for Audio-Visual Scene-Aware Dialog](https://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.html)
[7] [Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers](https://ojs.aaai.org/index.php/AAAI/article/view/16231)
* [Audio-Visual Scene-Aware Dialog and Reasoning Using Audio-Visual Transformers with Joint Student-Teacher Learning](https://ieeexplore.ieee.org/abstract/document/9746481)