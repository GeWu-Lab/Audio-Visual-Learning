# Motivation and Goal
## Event is not always audible and visible.
## Localizing the event boundary in temporal.


# Development Route
## Since the audio also reveals content-related information besides video, Tian et al. [1] first introduced the audio-visual event localization task whose aim is to temporally demarcate both audible and visible events from a video. They treated this task as a sequence labelling problem and developed an audio-driven visual attention mechanism to localize sounding objects in visual and designed a dual multi-modal residual network to integrate audio-visual features. 
## Subsequently, Lin et al. [2] used LSTM to solve the event localization in a sequence to sequence manner, integrating both global and local audio-visual information. 
## With a similar purpose that capturing global information, Wu et al. [3] designed the dual attention matching module to model both the high-level event information as well as local temporal information. 
## Besides, other attention mechanisms are following proposed to explore the inter- and intra-modal correlations [4,5,6]. 
## To further filter the irrelevant audio-visual pairs to avoid their interference with training, Zhou et al. [7] presented the Positive Sample Propagation (PSP) method to select audio-visual pairs with positive connections while ignoring the negative ones. 
## Further considering the noisy background, Xia et al. [4] designed the cross-modal time-level and event-level background suppression mechanism to alleviate the audio-visual inconsistency problem. 
## The audio-visual parsing task, which detects audible, visible and audible-visible events in the temporal dimension, is introduced to have a more fine-grained audio-visual scene understanding [8]. Tian et al. [8] formulated the audio-visual parsing as the multi-modal multiple instances learning in a weakly-supervised manner. They adopted a hybrid attention network and multi-modal multiple instance learning pooling method to aggregate and exploit the multi-modal temporal contexts, then discovered and mitigated the noisy label for each modality. 
## To generate more reliable uni-modal localization results, Wu et al. [9] individually predicted event labels for each modality via exchanging audio or visual track with other unrelated videos based on assumption that the prediction of the synthetic video would still be confident if the uni-modal signals indeed contain information of the target event. 
## In addition, Lin et al. [10] proposed to leverage the event semantics information across videos and the correlation between event categories to better distinguish and locate different events. 
## Later, more audio-visual parsing methods are presented with improvement at feature aggregation or attention module [11,12]. 


[1] [Audio-visual Event Localization in Unconstrained Videos](https://openaccess.thecvf.com/content_ECCV_2018/html/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.html)
[2] [Dual-modality Seq2Seq Network for Audio-visual Event Localization](https://ieeexplore.ieee.org/abstract/document/8683226/)
[3] [Dual Attention Matching for Audio-Visual Event Localization](https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.html)
[4] [Cross-Modal Attention Network for Temporal Inconsistent Audio-Visual Event Localization](https://ojs.aaai.org/index.php/AAAI/article/view/5361)
[5] [Audiovisual Transformer with Instance Attention for Audio-Visual Event Localization](https://openaccess.thecvf.com/content/ACCV2020/html/Lin_Audiovisual_Transformer_with_Instance_Attention_for_Audio-Visual_Event_Localization_ACCV_2020_paper.html)
[6] [Audio-Visual Event Localization via Recursive Fusion by Joint Co-Attention](https://openaccess.thecvf.com/content/WACV2021/html/Duan_Audio-Visual_Event_Localization_via_Recursive_Fusion_by_Joint_Co-Attention_WACV_2021_paper.html)
[7] [Positive Sample Propagation along the Audio-Visual Event Line](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Positive_Sample_Propagation_Along_the_Audio-Visual_Event_Line_CVPR_2021_paper.pdf)
[8] [Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing](https://link.springer.com/chapter/10.1007/978-3-030-58580-8_26)
[9] [Exploring Heterogeneous Clues for Weakly-Supervised Audio-Visual Video Parsing](https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Exploring_Heterogeneous_Clues_for_Weakly-Supervised_Audio-Visual_Video_Parsing_CVPR_2021_paper.html)
[10] [Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing](https://proceedings.neurips.cc/paper/2021/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html)
[11] [Investigating Modality Bias in Audio Visual Video Parsing](https://arxiv.org/abs/2203.16860)
[12] [Distributed Audio-Visual Parsing Based On Multimodal Transformer and Deep Joint Source Channel Coding](https://ieeexplore.ieee.org/abstract/document/9746660/)
