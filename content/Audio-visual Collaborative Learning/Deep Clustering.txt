# Motivation and Goal
## Audio-visual associations provides natural supervision. 


# Development Route
## Alwassel et al. [1] first proposed the Cross-Modal Deep Clustering (XDC) method to learning representation in a self-supervised manner. They trained the audio and visual model alternatively with the cross-modal pseudo label as the supervision, since the audio and visual modalities are tightly related in semantics and predicting the semantics of one modality based on another is promising. 
## While XDC method obtained single modality clusters, Asnao et al. [2] regarded the multiple modalities as different data augmentation forms, thus learnt multi-modal clusters to label the unlabelled videos. 
## Furthermore, Chen et al. [3] combines the pair-wise contrastive learning scheme and clustering methods, learning the audio-visual consistency at both instance-level as well as cluster-level to fully capture semantic information. These deep audio-visual clustering methods have worked well in learnt representation.


[1] [Self-Supervised Learning by Cross-Modal Audio-Video Clustering](https://proceedings.neurips.cc/paper/2020/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html)
[2] [Labelling Unlabelled Videos From Scratch With Multi-Modal Self-Supervision](https://proceedings.neurips.cc/paper/2020/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html)
[3] [Multimodal Clustering Networks for Self-Supervised Learning From Unlabeled Videos](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Multimodal_Clustering_Networks_for_Self-Supervised_Learning_From_Unlabeled_Videos_ICCV_2021_paper.html)