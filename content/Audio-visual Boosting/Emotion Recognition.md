# Motivation
Compared to the above recognition tasks that have clear classification criteria, the emotion recognition task is more difficult since the boundary between sentiments is even ambiguous for humans. The emotion recognition task pays attention to recognizing the sentiment of humans, which can be reflected in various aspects, including facial expression, audio, gestures even body movement. Facial expression and vocal contribute most of the human sentiment. Therefore, audio-visual input is important for the emotion recognition task.


# Demo
![emotion.mp4](content/videos/emotion.mp4)

[Video Source](https://vimeo.com/234955981)


# Overview of the field
- At the early stage of the emotion recognition study, features of multiple modalities are generated by hand-crafted technique. The audio features are often extracted based on the acoustic characteristic, like pitch and energy [1,2,3], while visual features are often based on facial texture (e.g., wrinkles, furrows) or components (e.g., eyebrows, mouth, eyes) [4]. The fusion operations of these studies can be categorized into feature-level fusion [5,6], decision-level fusion [7,8] and model-level fusion [9,10]. 
- However, the hand-crafted features rely on expert knowledge to design effective extraction methods. Even though, they are still with limited representation capability, and have difficulties in exploring multiple inter- and intra-modal correlations. 
- More recently, the more powerful deep neural network is widely employed in the emotion recognition task, and more diverse inter-, as well as intra-modal fusion strategies are proposed [11,12,13,14]. 
- Also, the transformer-based framework is introduced since its advantage in capturing global and local attention [15]. 
- Besides the above conventional emotion recognition, the sentiments with below-the-surface semantics, like sarcasm or humour, have also begun to consider multi-modal signals including visual expression and speech patterns for better capturing the crucial sentiment cues [16,17].


[1] [Emotion Recognition Based on Joint Visual and Audio Cues](https://ieeexplore.ieee.org/document/1699090)
[2] [Error Weighted Semi-Coupled Hidden Markov Model for Audio-Visual Emotion Recognition](https://ieeexplore.ieee.org/document/6042338)
[3] [Two-Level Hierarchical Alignment for Semi-Coupled HMM-Based Audiovisual Emotion Recognition With Temporal Course](https://ieeexplore.ieee.org/abstract/document/6542683)
[4] [Automatic Facial Expression Analysis: a Survey](https://www.sciencedirect.com/science/article/pii/S0031320302000523)
[5] [Context-sensitive Learning for Enhanced Audiovisual Emotion Classification](https://ieeexplore.ieee.org/document/7344611)
[6] [Audiovisual Vocal Outburst Classification in Noisy Acoustic Conditions](https://ieeexplore.ieee.org/document/6289067)
[7] [Decision Level Combination of Multiple Modalities for Recognition and Analysis of Emotional Expression](https://ieeexplore.ieee.org/abstract/document/549489)
[8] [Modeling Latent Discriminative Dynamic of Multi-dimensional Affective Signals](https://link.springer.com/chapter/10.1007/978-3-642-24571-8_51)
[9] [Audio-visual Affective Expression Recognition through Multistream Fused HMM][https://ieeexplore.ieee.org/document/4523967]
[10] [Audio Visual Emotion Recognition Based on Triple-Stream Dynamic Bayesian Network Models](https://link.springer.com/chapter/10.1007/978-3-642-24600-5_64)
[11] [Tensor Fusion Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/1707.07250)
[12] [Memory Fusion Network for Multi-view Sequential Learning](https://ojs.aaai.org/index.php/AAAI/article/view/12021)
[13] [Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7098709/)
[14] [Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences](https://openaccess.thecvf.com/content/CVPR2021/html/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.html)
[15] [A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis](https://aclanthology.org/2020.challengehml-1.1/)
[16] [Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis](https://aclanthology.org/2020.acl-main.401/)
[17] [Multi-modal Sarcasm Detection and Humor Classification in Code-mixed Conversations](https://ieeexplore.ieee.org/abstract/document/9442359)
